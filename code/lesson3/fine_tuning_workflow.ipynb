{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Large Language Models with LoRA/QLoRA\n",
        "\n",
        "Welcome! In this notebook, you'll learn how to fine-tune a large language model efficiently using Parameter-Efficient Fine-Tuning (PEFT) techniques.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to prepare and tokenize datasets for instruction fine-tuning\n",
        "- How to apply LoRA (Low-Rank Adaptation) to reduce trainable parameters\n",
        "- How to use QLoRA for memory-efficient training with quantization\n",
        "- How to implement assistant-only masking to train only on model responses\n",
        "- How to train, save, and share your fine-tuned model\n",
        "\n",
        "**Why this matters:**\n",
        "Fine-tuning large models typically requires massive computational resources. LoRA and QLoRA allow you to fine-tune models on consumer hardware by only training a small fraction of the parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Let's start by importing all the libraries we'll need. We're using:\n",
        "- **Transformers**: For loading models and tokenizers\n",
        "- **PEFT**: For applying LoRA adapters to our model\n",
        "- **Datasets**: For loading and processing training data\n",
        "- **Weights & Biases (wandb)**: For tracking training metrics (optional)\n",
        "- **BitsAndBytes**: For quantization if using QLoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install -q torch wandb datasets peft huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import torch\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from huggingface_hub import login\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def get_model_size_gb(model: torch.nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the model size in GB based on parameter count and data type.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "\n",
        "    Returns:\n",
        "        Model size in GB\n",
        "    \"\"\"\n",
        "    total_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        # Get the number of elements\n",
        "        param_size = param.numel()\n",
        "\n",
        "        # Get the size of each element in bytes based on data type\n",
        "        if param.dtype == torch.float32:\n",
        "            bytes_per_param = 4\n",
        "        elif param.dtype == torch.float16 or param.dtype == torch.bfloat16:\n",
        "            bytes_per_param = 2\n",
        "        elif param.dtype == torch.int8:\n",
        "            bytes_per_param = 1\n",
        "        elif param.dtype == torch.float64:\n",
        "            bytes_per_param = 8\n",
        "        else:\n",
        "            # Default to 4 bytes for unknown types\n",
        "            bytes_per_param = 4\n",
        "\n",
        "        total_size += param_size * bytes_per_param\n",
        "\n",
        "    # Convert bytes to GB (1 GB = 1024^3 bytes)\n",
        "    size_gb = total_size / (1024**3)\n",
        "    return size_gb\n",
        "\n",
        "\n",
        "def read_json_file(file_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Read a JSON file and return the contents as a dictionary.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def push_to_hub(\n",
        "    model: PeftModel, tokenizer: AutoTokenizer, model_name: str, hf_username: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Push a model and tokenizer to Hugging Face Hub.\n",
        "    \"\"\"\n",
        "    model_id = f\"{hf_username}/{model_name}\"\n",
        "    try:\n",
        "        model.push_to_hub(f\"{model_id}-adapters\", private=False)\n",
        "\n",
        "        merged_model = model.merge_and_unload()\n",
        "        merged_model.push_to_hub(model_id, private=False)\n",
        "\n",
        "        tokenizer.push_to_hub(model_id)\n",
        "        print(f\"Adapters successfully pushed to: https://huggingface.co/{model_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error pushing to Hugging Face: {e}\")\n",
        "        print(\"Make sure you're logged in with: huggingface-cli login\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"model_name\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"save_model_name\": \"llama-1b-legal-qlora\",\n",
        "    \"assistant_only_masking\": True,\n",
        "    \"use_qlora\": False,\n",
        "    \"deepspeed_version\": 1,\n",
        "    \"dataset_config\": {\n",
        "        \"dataset_dir_path\": \"../../data\",\n",
        "        \"instruction_column\": None,\n",
        "        \"input_column\": \"question\",\n",
        "        \"output_column\": \"answer\",\n",
        "        \"max_length\": 2048,\n",
        "        \"sample_size\": None,\n",
        "        \"validation_size\": None,\n",
        "        \"test_size\": None\n",
        "    },\n",
        "    \"quantization_config\": {\n",
        "        \"load_in_4bit\": True\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "        \"r\": 8,\n",
        "        \"lora_alpha\": 32,\n",
        "        \"target_modules\": [\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\"\n",
        "        ],\n",
        "        \"lora_dropout\": 0.05,\n",
        "        \"bias\": \"none\"\n",
        "    },\n",
        "    \"training_args\": {\n",
        "        \"output_dir\": \"./checkpoints\",\n",
        "        \"per_device_train_batch_size\": 4,\n",
        "        \"num_train_epochs\": 2,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"logging_steps\": 4,\n",
        "        \"save_strategy\": \"epoch\",\n",
        "        \"eval_strategy\": \"epoch\",\n",
        "        \"warmup_steps\": 0,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        \"optim\": \"adamw_torch\",\n",
        "        \"report_to\": \"wandb\",\n",
        "        \"remove_unused_columns\": False,\n",
        "        \"dataloader_drop_last\": True,\n",
        "        \"gradient_checkpointing\": False,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"metric_for_best_model\": \"eval_loss\",\n",
        "        \"greater_is_better\": False,\n",
        "        \"fp16\": False,\n",
        "        \"logging_dir\": \"./logs\",\n",
        "        \"logging_first_step\": True,\n",
        "        \"log_level\": \"info\",\n",
        "        \"disable_tqdm\": False\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"early_stopping_patience\": 2,\n",
        "        \"early_stopping_threshold\": 0.05\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication and Configuration\n",
        "\n",
        "Before we begin training, we need to authenticate with HuggingFace (to download models and upload results) and optionally with Weights & Biases (to track training progress).\n",
        "\n",
        "**Important:** Make sure you have a `.env` file with:\n",
        "- `HF_TOKEN`: Your HuggingFace access token\n",
        "- `HF_USERNAME`: Your HuggingFace username\n",
        "- `WANDB_API_KEY`: Your Weights & Biases API key (optional)\n",
        "\n",
        "We'll also load our training configuration from `config.json`, which contains all hyperparameters and settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-1-lora-experiment</strong> at: <a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune/runs/z0a9rstb' target=\"_blank\">https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune/runs/z0a9rstb</a><br> View project at: <a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune' target=\"_blank\">https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251104_050343-z0a9rstb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week3/code/lesson3/wandb/run-20251104_050838-9srgjziq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune/runs/9srgjziq' target=\"_blank\">run-1-lora-experiment</a></strong> to <a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune' target=\"_blank\">https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune/runs/9srgjziq' target=\"_blank\">https://wandb.ai/mohamedabdelhamid3030/my-llm-finetune/runs/9srgjziq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "HF_USERNAME = os.getenv(\"HF_USERNAME\")\n",
        "login(HF_TOKEN)\n",
        "\n",
        "\n",
        "if config[\"training_args\"][\"report_to\"] == \"wandb\":\n",
        "    if os.getenv(\"WANDB_API_KEY\") is None:\n",
        "        raise ValueError(\"WANDB_API_KEY is not set in the environment variables\")\n",
        "    \n",
        "    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
        "    wandb.init(\n",
        "        project=\"my-llm-finetune\",\n",
        "        name=\"run-1-lora-experiment\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: meta-llama/Llama-3.2-1B-Instruct\n",
            "Using QLoRA: False\n",
            "Assistant-only masking: True\n"
          ]
        }
      ],
      "source": [
        "model_name = config[\"model_name\"]\n",
        "dataset_config = config[\"dataset_config\"]\n",
        "quantization_config = config[\"quantization_config\"]\n",
        "lora_config = config[\"lora_config\"]\n",
        "use_qlora = config[\"use_qlora\"]\n",
        "training_args = config[\"training_args\"]\n",
        "save_model_name = config[\"save_model_name\"]\n",
        "assistant_only_masking = config[\"assistant_only_masking\"]\n",
        "early_stopping_config = config.get(\"early_stopping\", {})\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Using QLoRA: {use_qlora}\")\n",
        "print(f\"Assistant-only masking: {assistant_only_masking}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Preparation Functions\n",
        "\n",
        "Now we'll define the functions that prepare our data for training. These functions will:\n",
        "\n",
        "1. **Load and format the dataset** into an instruction-following format\n",
        "2. **Apply assistant-only masking** so the model only learns from the output portion\n",
        "3. **Tokenize the data** to convert text into numerical tokens the model can process\n",
        "4. **Create batches** with proper padding\n",
        "\n",
        "**Key Concept - Assistant-Only Masking:**\n",
        "When fine-tuning instruction-following models, we typically only want the model to learn to predict the assistant's response, not the instruction itself. We achieve this by masking the instruction tokens (setting them to -100), so they don't contribute to the loss during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(\n",
        "    input_column: str,\n",
        "    output_column: str,\n",
        "    instruction_column: str = None,\n",
        "    default_instruction: str = None,\n",
        "    dataset_name: str = None,\n",
        "    dataset_dir_path: str = None,\n",
        "    sample_size: Optional[int] = None,\n",
        "    validation_size: Optional[float] = None,\n",
        "    test_size: Optional[float] = None,\n",
        ") -> Tuple[Dataset, Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Load and prepare the dataset for fine-tuning.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset from HuggingFace\n",
        "        instruction_column: Column name for instructions\n",
        "        input_column: Column name for inputs\n",
        "        output_column: Column name for outputs\n",
        "        sample_size: Optional limit on training samples\n",
        "        validation_size: Fraction of data for validation\n",
        "        test_size: Fraction of data for testing\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (train_dataset, validation_dataset, test_dataset)\n",
        "    \"\"\"\n",
        "    if dataset_dir_path is not None:\n",
        "        train_file_path = os.path.join(dataset_dir_path, \"train_data.jsonl\")\n",
        "        validation_file_path = os.path.join(dataset_dir_path, \"validation_data.jsonl\")\n",
        "        test_file_path = os.path.join(dataset_dir_path, \"test_data.jsonl\")\n",
        "\n",
        "        dataset = load_dataset(\n",
        "            \"json\",\n",
        "            data_files={\n",
        "                \"train\": train_file_path,\n",
        "                \"validation\": validation_file_path,\n",
        "                \"test\": test_file_path\n",
        "            }\n",
        "        )\n",
        "\n",
        "    elif dataset_name is not None:\n",
        "        dataset = load_dataset(dataset_name)\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(\"Either dataset_dir_path or dataset_name must be provided\")\n",
        "    \n",
        "    def format_instruction_data(data_point: Dict) -> str:\n",
        "        if instruction_column is not None:\n",
        "            instruction = data_point[instruction_column]\n",
        "        else:\n",
        "            instruction = default_instruction\n",
        "\n",
        "        input_text = data_point[input_column]\n",
        "        output = data_point[output_column]\n",
        "        \n",
        "        formatted_text = f\"### Instruction\\n{instruction}\\n\\n\"\n",
        "        \n",
        "        if input_text:\n",
        "            formatted_text += f\"### Input\\n{input_text}\\n\\n\"\n",
        "        \n",
        "        formatted_text += f\"### Output\\n{output}\"\n",
        "        \n",
        "        return {\"text\": formatted_text}\n",
        "    \n",
        "    \n",
        "    \n",
        "    if sample_size is not None:\n",
        "        dataset[\"train\"] = dataset[\"train\"].select(range(sample_size))\n",
        "    \n",
        "    if validation_size is not None and test_size is not None:\n",
        "        val_plus_test_size = validation_size + test_size\n",
        "        split = dataset[\"train\"].train_test_split(test_size=val_plus_test_size, seed=42)\n",
        "        dataset[\"validation_and_test\"] = split[\"test\"]\n",
        "        dataset[\"train\"] = split[\"train\"]\n",
        "        \n",
        "        test_to_val_plus_test_ratio = test_size / val_plus_test_size\n",
        "        split = dataset[\"validation_and_test\"].train_test_split(\n",
        "            test_size=test_to_val_plus_test_ratio, seed=42\n",
        "        )\n",
        "        dataset[\"test\"] = split[\"test\"]\n",
        "        dataset[\"validation\"] = split[\"train\"]\n",
        "        del dataset[\"validation_and_test\"]\n",
        "    \n",
        "    elif validation_size is not None:\n",
        "        split = dataset[\"train\"].train_test_split(test_size=validation_size, seed=42)\n",
        "        dataset[\"validation\"] = split[\"test\"]\n",
        "        dataset[\"train\"] = split[\"train\"]\n",
        "    \n",
        "    elif test_size is not None:\n",
        "        split = dataset[\"train\"].train_test_split(test_size=test_size, seed=42)\n",
        "        dataset[\"test\"] = split[\"test\"]\n",
        "        dataset[\"train\"] = split[\"train\"]\n",
        "    \n",
        "    validation_dataset = None\n",
        "    test_dataset = None\n",
        "    \n",
        "    train_dataset = dataset[\"train\"].map(\n",
        "        format_instruction_data, desc=\"Formatting train data\"\n",
        "    )\n",
        "    \n",
        "    if \"validation\" in dataset:\n",
        "        validation_dataset = dataset[\"validation\"].map(\n",
        "            format_instruction_data, desc=\"Formatting validation data\"\n",
        "        )\n",
        "    \n",
        "    if \"test\" in dataset:\n",
        "        test_dataset = dataset[\"test\"].map(\n",
        "            format_instruction_data, desc=\"Formatting test data\"\n",
        "        )\n",
        "    \n",
        "    return train_dataset, validation_dataset, test_dataset\n",
        "\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset = prepare_dataset(\n",
        "    dataset_dir_path=dataset_config[\"dataset_dir_path\"],\n",
        "    input_column=dataset_config[\"input_column\"],\n",
        "    output_column=dataset_config[\"output_column\"],\n",
        "    default_instruction=\"You are a helpful assistant responsible for solving simple math problems. Answer the following question by stating you reasoning then provide the answer at the end preceeded by ####.\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_assistant_masking(\n",
        "    input_ids: List[int], tokenizer: AutoTokenizer\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Apply assistant-only masking by setting instruction tokens to -100.\n",
        "    This ensures the model only learns from the output/response portion.\n",
        "    \n",
        "    Args:\n",
        "        input_ids: Tokenized input sequence\n",
        "        tokenizer: The tokenizer used\n",
        "    \n",
        "    Returns:\n",
        "        Labels with instruction tokens masked (-100)\n",
        "    \"\"\"\n",
        "    labels = input_ids.copy()\n",
        "    \n",
        "    output_marker = \"### Output\"\n",
        "    output_marker_tokens = tokenizer.encode(output_marker, add_special_tokens=False)\n",
        "    \n",
        "    output_start_idx = None\n",
        "    for i in range(len(input_ids) - len(output_marker_tokens) + 1):\n",
        "        if input_ids[i : i + len(output_marker_tokens)] == output_marker_tokens:\n",
        "            output_start_idx = i + len(output_marker_tokens)\n",
        "            break\n",
        "    \n",
        "    if output_start_idx is not None:\n",
        "        for i in range(output_start_idx):\n",
        "            labels[i] = -100\n",
        "    \n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCollatorForCausalLM:\n",
        "    \"\"\"\n",
        "    Custom data collator for causal language modeling.\n",
        "    Handles padding of input_ids, attention_mask, and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __call__(self, features):\n",
        "        labels = [f.pop(\"labels\") for f in features]\n",
        "        \n",
        "        batch = self.tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
        "        \n",
        "        max_len = batch[\"input_ids\"].size(1)\n",
        "        padded_labels = torch.full((len(labels), max_len), -100, dtype=torch.long)\n",
        "        for i, l in enumerate(labels):\n",
        "            padded_labels[i, : len(l)] = torch.tensor(l, dtype=torch.long)\n",
        "        batch[\"labels\"] = padded_labels\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_dataset(\n",
        "    model_name: str,\n",
        "    train_dataset: Dataset,\n",
        "    validation_dataset: Optional[Dataset] = None,\n",
        "    test_dataset: Optional[Dataset] = None,\n",
        "    assistant_only_masking: bool = True,\n",
        "    max_length: int = 2048,\n",
        ") -> Tuple[Dataset, Dataset, Dataset, AutoTokenizer]:\n",
        "    \"\"\"\n",
        "    Load and tokenize the dataset with optional assistant-only masking.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the model to use for tokenizer\n",
        "        dataset_name: Name of the dataset to use\n",
        "        assistant_only_masking: Whether to apply assistant-only masking\n",
        "        max_length: Maximum sequence length\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (train, validation, test, tokenizer)\n",
        "    \"\"\"    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    \n",
        "    def tokenize_and_mask_function(examples):\n",
        "        texts_with_eos = [text + tokenizer.eos_token for text in examples[\"text\"]]\n",
        "        tokenized = tokenizer(\n",
        "            texts_with_eos,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=max_length,\n",
        "            return_tensors=None,\n",
        "            add_special_tokens=True,\n",
        "        )\n",
        "        \n",
        "        labels = []\n",
        "        for input_ids in tokenized[\"input_ids\"]:\n",
        "            if assistant_only_masking:\n",
        "                masked_labels = apply_assistant_masking(input_ids, tokenizer)\n",
        "            else:\n",
        "                masked_labels = input_ids\n",
        "            labels.append(masked_labels)\n",
        "        \n",
        "        tokenized[\"labels\"] = labels\n",
        "        return tokenized\n",
        "    \n",
        "    train = train_dataset.map(\n",
        "        tokenize_and_mask_function,\n",
        "        batched=True,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "        desc=\"Processing train dataset\",\n",
        "    )\n",
        "    \n",
        "    validation = None\n",
        "    if validation_dataset is not None:\n",
        "        validation = validation_dataset.map(\n",
        "            tokenize_and_mask_function,\n",
        "            batched=True,\n",
        "            remove_columns=train_dataset.column_names,\n",
        "            desc=\"Processing validation dataset\",\n",
        "        )\n",
        "    \n",
        "    test = None\n",
        "    if test_dataset is not None:\n",
        "        test = test_dataset.map(\n",
        "            tokenize_and_mask_function,\n",
        "            batched=True,\n",
        "            remove_columns=train_dataset.column_names,\n",
        "            desc=\"Processing test dataset\",\n",
        "        )\n",
        "    \n",
        "    return train, validation, test, tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Prepare Dataset\n",
        "\n",
        "Time to load and process our training data! This cell will:\n",
        "- Download the dataset from HuggingFace\n",
        "- Format it into the instruction format\n",
        "- Tokenize all examples\n",
        "- Apply masking if enabled\n",
        "- Split into train/validation/test sets\n",
        "\n",
        "This might take a few minutes depending on your dataset size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fffec0ada46c4b6cbef62f9e7bfafb9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "505a648ed29b4450b79bbc8b5b71dac0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing validation dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e8bf32bf0c94bd5af759664e9513c62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing test dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 7473\n",
            "Validation dataset size: 400\n",
            "Test dataset size: 400\n"
          ]
        }
      ],
      "source": [
        "train, validation, test, tokenizer = tokenize_dataset(\n",
        "    model_name=model_name,\n",
        "    train_dataset=train_dataset,\n",
        "    validation_dataset=validation_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    assistant_only_masking=assistant_only_masking,\n",
        "    max_length=dataset_config.get(\"max_length\", 2048),\n",
        ")\n",
        "\n",
        "print(f\"Train dataset size: {len(train)}\")\n",
        "print(f\"Validation dataset size: {len(validation) if validation else 0}\")\n",
        "print(f\"Test dataset size: {len(test) if test else 0}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inspect Dataset Sample\n",
        "\n",
        "Let's take a peek at our processed data to verify everything looks correct. \n",
        "\n",
        "If you enabled assistant-only masking, you'll see statistics showing:\n",
        "- How many tokens are in each sequence\n",
        "- How many tokens are masked (the instruction part)\n",
        "- How many tokens we're actually training on (the response part)\n",
        "\n",
        "**What to expect:** Typically, 50-70% of tokens will be masked, as instructions are usually shorter than responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 129\n",
            "Masked tokens: 79\n",
            "Training tokens: 50\n",
            "Mask ratio: 61.24%\n"
          ]
        }
      ],
      "source": [
        "if assistant_only_masking:\n",
        "    first_example = train[0]\n",
        "    input_ids = first_example[\"input_ids\"]\n",
        "    labels = first_example[\"labels\"]\n",
        "    \n",
        "    masked_tokens = sum(1 for label in labels if label == -100)\n",
        "    total_tokens = len(labels)\n",
        "    \n",
        "    print(f\"Total tokens: {total_tokens}\")\n",
        "    print(f\"Masked tokens: {masked_tokens}\")\n",
        "    print(f\"Training tokens: {total_tokens - masked_tokens}\")\n",
        "    print(f\"Mask ratio: {masked_tokens/total_tokens:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Configuration\n",
        "\n",
        "Now we'll configure our Parameter-Efficient Fine-Tuning (PEFT) approach.\n",
        "\n",
        "**LoRA (Low-Rank Adaptation):**\n",
        "Instead of updating all model parameters, LoRA adds small trainable matrices to specific layers. This dramatically reduces the number of parameters you need to train (often by 1000x or more!).\n",
        "\n",
        "**QLoRA (Quantized LoRA):**\n",
        "If enabled, we'll load the model in 4-bit or 8-bit precision, further reducing memory requirements. This allows you to fine-tune larger models on smaller GPUs.\n",
        "\n",
        "**Key hyperparameters:**\n",
        "- `r` (rank): Size of the low-rank matrices (higher = more capacity, more memory)\n",
        "- `lora_alpha`: Scaling factor for LoRA updates\n",
        "- `target_modules`: Which model layers to apply LoRA to (typically attention and feed-forward layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA Configuration:\n",
            "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'k_proj', 'q_proj', 'o_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)\n"
          ]
        }
      ],
      "source": [
        "bnb_config = None\n",
        "\n",
        "if use_qlora:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=quantization_config[\"load_in_4bit\"],\n",
        "        load_in_8bit=not quantization_config[\"load_in_4bit\"],\n",
        "    )\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    **lora_config,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load Model with PEFT\n",
        "\n",
        "Time to load our base model and apply the LoRA adapters!\n",
        "\n",
        "This step will:\n",
        "1. Download the base model from HuggingFace (if not cached)\n",
        "2. Apply quantization if using QLoRA\n",
        "3. Inject LoRA adapter layers into the model\n",
        "4. Freeze the base model parameters (only adapters will be trained)\n",
        "\n",
        "**Watch for:** The model will print how many parameters are trainable vs. frozen. You should see that only 0.1-1% of parameters are trainable with LoRA. This is the magic that makes efficient fine-tuning possible!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PEFT model size: 4.61 GB\n",
            "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
          ]
        }
      ],
      "source": [
        "def get_apply_peft(\n",
        "    model_name: str,\n",
        "    lora_config: LoraConfig,\n",
        "    qlora_config: Optional[BitsAndBytesConfig] = None,\n",
        ") -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Load model and apply PEFT (LoRA) configuration.\n",
        "    \n",
        "    Args:\n",
        "        model_name: HuggingFace model name\n",
        "        lora_config: LoRA configuration\n",
        "        qlora_config: Optional quantization configuration\n",
        "    \n",
        "    Returns:\n",
        "        Model with PEFT adapters applied\n",
        "    \"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, quantization_config=qlora_config, device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    return get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "peft_model = get_apply_peft(model_name, lora_config, bnb_config)\n",
        "\n",
        "print(f\"PEFT model size: {get_model_size_gb(peft_model):.2f} GB\")\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Setup\n",
        "\n",
        "Almost ready to train! Let's configure the training process.\n",
        "\n",
        "We're setting up:\n",
        "- **Training arguments**: Learning rate, batch size, number of epochs, etc.\n",
        "- **Data collator**: Handles batching and padding during training\n",
        "- **Callbacks**: Optional early stopping to prevent overfitting\n",
        "\n",
        "**Key training parameters to understand:**\n",
        "- `learning_rate`: How big of steps to take during optimization (typically 1e-4 to 5e-4 for LoRA)\n",
        "- `per_device_train_batch_size`: Number of examples per GPU (adjust based on your memory)\n",
        "- `gradient_accumulation_steps`: Simulate larger batches if memory is limited\n",
        "- `num_train_epochs`: How many times to go through the entire dataset\n",
        "- `save_steps` / `eval_steps`: How often to save checkpoints and evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up training...\n",
            "Training configuration complete\n"
          ]
        }
      ],
      "source": [
        "print(\"Setting up training...\")\n",
        "\n",
        "training_args = TrainingArguments(**training_args)\n",
        "data_collator = DataCollatorForCausalLM(tokenizer)\n",
        "\n",
        "callbacks = []\n",
        "if early_stopping_config:\n",
        "    early_stopping_callback = EarlyStoppingCallback(**early_stopping_config)\n",
        "    callbacks.append(early_stopping_callback)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=validation,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=callbacks if callbacks else None,\n",
        ")\n",
        "\n",
        "print(\"Training configuration complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Train the Model\n",
        "\n",
        "Here we go! Time to train your model!\n",
        "\n",
        "**What happens during training:**\n",
        "- The model processes batches of your training data\n",
        "- It makes predictions and compares them to the correct outputs\n",
        "- It updates the LoRA adapter weights to improve its predictions\n",
        "- Every few steps, it evaluates on your validation set to track progress\n",
        "\n",
        "**What you'll see:**\n",
        "- Loss values (lower is better - measures prediction error)\n",
        "- Training speed (samples/second)\n",
        "- Periodic evaluation results\n",
        "- Progress bar showing completion\n",
        "\n",
        "**How long will this take?** Depends on:\n",
        "- Your dataset size (more data = longer training)\n",
        "- Your GPU (faster GPU = faster training)\n",
        "- Number of epochs and batch size\n",
        "\n",
        "Grab a coffee - this could take anywhere from minutes to hours!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save and Share Your Model\n",
        "\n",
        "Congratulations! Your model is trained. Now let's save it and share it with the world (or your team).\n",
        "\n",
        "**What we're saving:**\n",
        "- Only the LoRA adapter weights (not the entire base model!)\n",
        "- The tokenizer configuration\n",
        "- Training metadata\n",
        "\n",
        "**Why this is cool:**\n",
        "Instead of saving a 7GB+ model, we're only saving ~10-100MB of adapter weights. Anyone can then:\n",
        "1. Download the base model\n",
        "2. Load your adapters on top\n",
        "3. Use your fine-tuned model\n",
        "\n",
        "**Next steps after this notebook:**\n",
        "- Test your model's performance on held-out data\n",
        "- Compare it to the base model\n",
        "- Iterate on your dataset or hyperparameters if needed\n",
        "- Share your model on HuggingFace Hub for others to use!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_adapter_path = \"model-adapters\"\n",
        "peft_model.save_pretrained(local_adapter_path)\n",
        "tokenizer.save_pretrained(local_adapter_path)\n",
        "\n",
        "print(f\"Adapters saved locally to: {local_adapter_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "push_to_hub(peft_model, tokenizer, save_model_name, HF_USERNAME)\n",
        "\n",
        "print(\"Training complete and model pushed to HuggingFace Hub\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
