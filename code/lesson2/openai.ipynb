{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303fd5c5",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-4o-mini on SAMSUM dataset\n",
    "\n",
    "\n",
    "This notebook demonstrates how to fine-tune OpenAI's GPT-4o-mini model.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e777d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q openai datasets python-dotenv evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0effff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "train_data_path = os.path.join(DATA_DIR, \"train_data.jsonl\")\n",
    "val_data_path = os.path.join(DATA_DIR, \"validation_data.jsonl\")\n",
    "test_data_path = os.path.join(DATA_DIR, \"test_data.jsonl\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba527c6",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b53ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4395461c949c8bb237969ce3f7df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/samsum\")\n",
    "\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "val_data = dataset['validation'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for sample in data:\n",
    "            f.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"## Dialogue:\\n{example['dialogue']}\\n## Summary:\\n\"\n",
    "\n",
    "val_data = val_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "test_data = test_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "train_data = train_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "\n",
    "save_jsonl(val_data, val_data_path)\n",
    "save_jsonl(test_data, test_data_path)\n",
    "save_jsonl(train_data, train_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4becc",
   "metadata": {},
   "source": [
    "## Evaluate Base Model\n",
    "\n",
    "Test GPT-4o-mini's baseline accuracy on the validation set (no fine-tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a45efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:47<00:00,  4.24it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6925f807f1949faa55ffb9e4bfc8e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base GPT-4o-mini accuracy on validation set:\n",
      "Rouge-1: 12.24%\n",
      "Rouge-2: 1.41%\n",
      "Rouge-L: 8.85%\n"
     ]
    }
   ],
   "source": [
    "def calculate_rouge(generated_texts, true_summary):\n",
    "  rouge = evaluate.load(\"rouge\")\n",
    "  results = rouge.compute(predictions=generated_texts, references=true_summary)\n",
    "  return results\n",
    "\n",
    "\n",
    "def process_sample(sample, retries=5):\n",
    "    \"\"\"Processes one sample, retrying on rate limit errors.\"\"\"\n",
    "    q = sample[\"dialogue\"]\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Dialogue: {q}\\nSummary:\"}],\n",
    "                temperature=0\n",
    "            )\n",
    "            pred = completion.choices[0].message.content\n",
    "            return pred\n",
    "\n",
    "        except RateLimitError:\n",
    "            sleep_time = 2 ** attempt + 0.5  # exponential backoff\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            # catch-all for unexpected errors\n",
    "            print(f\"Error processing sample: {e}\")\n",
    "            return False\n",
    "\n",
    "    return False  # if all retries failed\n",
    "\n",
    "\n",
    "# Parallel execution with rate-limit handling\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(process_sample, sample): sample for sample in val_data}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Worker failed: {e}\")\n",
    "            results.append(False)\n",
    "\n",
    "rouge = calculate_rouge(results, list(val_data[\"summary\"]))\n",
    "print(f\"\\nBase GPT-4o-mini accuracy on validation set:\\nRouge-1: {rouge['rouge1']:.2%}\\nRouge-2: {rouge['rouge2']:.2%}\\nRouge-L: {rouge['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef4518",
   "metadata": {},
   "source": [
    "## Convert to OpenAI Format\n",
    "\n",
    "Convert the dataset to JSONL format with the required `messages` structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b1486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation files created successfully\n"
     ]
    }
   ],
   "source": [
    "# Convert training data to JSONL\n",
    "with open(\"samsum_train.jsonl\", \"w\") as f:\n",
    "    for sample in train_data:\n",
    "        record = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"dialogue\"]},\n",
    "                {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Convert validation data to JSONL\n",
    "with open(\"samsum_val.jsonl\", \"w\") as f:\n",
    "    for sample in val_data:\n",
    "        record = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"dialogue\"]},\n",
    "                {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(\"Training and validation files created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31158d21",
   "metadata": {},
   "source": [
    "## Upload Data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84557ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Training file ID: file-8c9WvyB6m36btbELA1mFUw\n",
      "Uploading validation file...\n",
      "Validation file ID: file-4UfRHK4YfzGnFQVu8ViTuk\n"
     ]
    }
   ],
   "source": [
    "# Upload training file\n",
    "print(\"Uploading training file...\")\n",
    "train_file = client.files.create(\n",
    "    file=open(\"samsum_train.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Training file ID: {train_file.id}\")\n",
    "\n",
    "# Upload validation file\n",
    "print(\"Uploading validation file...\")\n",
    "val_file = client.files.create(\n",
    "    file=open(\"samsum_val.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Validation file ID: {val_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be00d8",
   "metadata": {},
   "source": [
    "## Create Fine-tuning Job\n",
    "\n",
    "Start the fine-tuning job with 1 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49e02500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating fine-tuning job...\n",
      "Fine-tuning job created: ftjob-T7IbDm0sdmzGfxnTZEVsrygf\n",
      "Status: validating_files\n",
      "Fine-tuning job created: ftjob-EDLh25wsS2ouLC4FC2WBPI3A\n",
      "Status: validating_files\n",
      "Fine-tuning job created: ftjob-EDLh25wsS2ouLC4FC2WBPI3A\n",
      "Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "# Create fine-tuning job\n",
    "print(\"\\nCreating fine-tuning job...\")\n",
    "job1 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 1,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created: {job1.id}\")\n",
    "print(f\"Status: {job1.status}\")\n",
    "\n",
    "\n",
    "job2 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 0.5,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created: {job2.id}\")\n",
    "print(f\"Status: {job2.status}\")\n",
    "\n",
    "job3 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 0.1,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created: {job2.id}\")\n",
    "print(f\"Status: {job2.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e4b4e",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "Test the fine-tuned model on the validation set and compare to baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "786d7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_id):\n",
    "    def evaluate_sample(sample):\n",
    "        q = sample[\"dialogue\"]\n",
    "        gt = sample[\"summary\"]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Dialogue: {q}\\nSummary:\"}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        pred = completion.choices[0].message.content\n",
    "\n",
    "        return pred\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(evaluate_sample, s) for s in val_data]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(f.result())\n",
    "\n",
    "\n",
    "    rouge = calculate_rouge(results, list(val_data[\"summary\"])) \n",
    "    return rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "914b5203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Victoria: God I'm really broke, I spent way to much this month ðŸ˜«\\nVictoria: At least we get paid soon..\\nMagda: Yeah, don't remind me, I know the feeling\\nMagda: I just paid my car insurance, I feel robbed ðŸ˜‚\\nVictoria: Thankfully mine is paid for the rest of the year ðŸ™\\nMagda: ðŸ‘Œ\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = val_data[0]['dialogue']\n",
    "dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82cfa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magda and Victoria feel broke. '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = val_data[0]['summary']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845c388",
   "metadata": {},
   "source": [
    "## lr_multiplier = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68eafed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:33<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.11296262397526508\n",
      "Rouge-2:  0.01201384855827984\n",
      "Rouge-L:  0.0882997597030008\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rgruH')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d38f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Victoria and Magda are both feeling financially strained this month. Magda just paid her car insurance, while Victoria's is paid for the rest of the year.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rgruH',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c910371b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victoria expresses her frustration about being broke after overspending, but looks forward to getting paid soon. Magda relates to her feelings and mentions just paying her car insurance, feeling like she was robbed. Victoria is relieved that her car insurance is already paid for the year.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d38353",
   "metadata": {},
   "source": [
    "## lr_multiplier = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c45ee459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:26<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.12924997802793536\n",
      "Rouge-2:  0.02352655682291644\n",
      "Rouge-L:  0.10569437415041297\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4qjCPA')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3093be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victoria and Magda are broke. Magda just paid her car insurance.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4qjCPA',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ace933",
   "metadata": {},
   "source": [
    "## lr_multiplier = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "190968ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:37<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.1149850396819559\n",
      "Rouge-2:  0.0124089839747357\n",
      "Rouge-L:  0.09043878235459887\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rbTPa')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03f977a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Victoria and Magda are broke. Victoria's car insurance is paid for the rest of the year.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rbTPa',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff980c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
