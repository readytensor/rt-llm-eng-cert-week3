{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303fd5c5",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-4o-mini on SAMSUM dataset\n",
    "\n",
    "\n",
    "This notebook demonstrates how to fine-tune OpenAI's GPT-4o-mini model.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e777d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai datasets python-dotenv evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0effff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "DATA_DIR = \"./\"\n",
    "train_data_path = os.path.join(DATA_DIR, \"train_data.jsonl\")\n",
    "val_data_path = os.path.join(DATA_DIR, \"validation_data.jsonl\")\n",
    "test_data_path = os.path.join(DATA_DIR, \"test_data.jsonl\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba527c6",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b53ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"knkarthick/samsum\")\n",
    "\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "val_data = dataset['validation'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for sample in data:\n",
    "            f.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"## Dialogue:\\n{example['dialogue']}\\n## Summary:\\n\"\n",
    "\n",
    "val_data = val_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "test_data = test_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "train_data = train_data.map(lambda ex: {\"text\": format_prompt(ex)})\n",
    "\n",
    "save_jsonl(val_data, val_data_path)\n",
    "save_jsonl(test_data, test_data_path)\n",
    "save_jsonl(train_data, train_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4becc",
   "metadata": {},
   "source": [
    "## Evaluate Base Model\n",
    "\n",
    "Test GPT-4o-mini's baseline accuracy on the validation set (no fine-tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a45efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(generated_texts, true_summary):\n",
    "  rouge = evaluate.load(\"rouge\")\n",
    "  results = rouge.compute(predictions=generated_texts, references=true_summary)\n",
    "  return results\n",
    "\n",
    "def build_user_prompt(dialog):\n",
    "    prompt = (                \n",
    "        \"You are a helpful assistant who writes concise, factual summaries of conversations. \"\n",
    "        \"Summarize the following conversation into a single sentence. \"\n",
    "        \"If it can summarized in a short sentence, do it. \\n\"\n",
    "        f\"## Dialog: {dialog}\\n\"\n",
    "        \"## Summary:\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def process_sample(sample, retries=5):\n",
    "    \"\"\"Processes one sample, retrying on rate limit errors.\"\"\"\n",
    "    dialog = sample[\"dialogue\"]\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": build_user_prompt(dialog)}],\n",
    "                temperature=0\n",
    "            )\n",
    "            pred = completion.choices[0].message.content\n",
    "            return pred\n",
    "\n",
    "        except RateLimitError:\n",
    "            sleep_time = 2 ** attempt + 0.5  # exponential backoff\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            # catch-all for unexpected errors\n",
    "            print(f\"Error processing sample: {e}\")\n",
    "            return False\n",
    "\n",
    "    return False  # if all retries failed\n",
    "\n",
    "def process_all_samples(samples):\n",
    "    \"\"\"Run samples in parallel while preserving original order.\"\"\"\n",
    "    results = [None] * len(samples)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_sample, sample): idx\n",
    "            for idx, sample in enumerate(samples)\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing samples\"):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Worker failed at index {idx}: {e}\")\n",
    "                results[idx] = None\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff25872-ef7a-46d9-b4b1-3fbf7f3ea084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 200/200 [00:40<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base GPT-4o-mini ROUGE score on validation set:\n",
      "Rouge-1: 40.48%\n",
      "Rouge-2: 15.93%\n",
      "Rouge-L: 32.55%\n"
     ]
    }
   ],
   "source": [
    "results = process_all_samples(samples=val_data)\n",
    "\n",
    "rouge = calculate_rouge(results, list(val_data[\"summary\"]))\n",
    "print(\n",
    "    f\"\\nBase GPT-4o-mini ROUGE score on validation set:\\n\"\n",
    "    f\"Rouge-1: {rouge['rouge1']:.2%}\\n\"\n",
    "    f\"Rouge-2: {rouge['rouge2']:.2%}\\n\"\n",
    "    f\"Rouge-L: {rouge['rougeL']:.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1e9a5a-ec98-4775-8792-c6a426599d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Victoria and Magda discuss their financial struggles, with Victoria feeling broke after overspending and Magda expressing frustration after paying her car insurance, while Victoria is relieved that hers is already paid for the year.',\n",
       " 'Magda and Victoria feel broke. ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0], val_data[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef4518",
   "metadata": {},
   "source": [
    "## Convert to OpenAI Format\n",
    "\n",
    "Convert the dataset to JSONL format with the required `messages` structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b1486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation files created successfully\n"
     ]
    }
   ],
   "source": [
    "# Convert training data to JSONL\n",
    "with open(\"samsum_train.jsonl\", \"w\") as f:\n",
    "    for sample in train_data:\n",
    "        record = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": build_user_prompt(sample[\"dialogue\"])},\n",
    "                {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Convert validation data to JSONL\n",
    "with open(\"samsum_val.jsonl\", \"w\") as f:\n",
    "    for sample in val_data:\n",
    "        record = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": build_user_prompt(sample[\"dialogue\"])},\n",
    "                {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(\"Training and validation files created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31158d21",
   "metadata": {},
   "source": [
    "## Upload Data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84557ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Training file ID: file-SNXVCoSrGDpvnARDNR84Zq\n",
      "Uploading validation file...\n",
      "Validation file ID: file-S3jojKW4n7QxahNcECkCdh\n"
     ]
    }
   ],
   "source": [
    "# Upload training file\n",
    "print(\"Uploading training file...\")\n",
    "train_file = client.files.create(\n",
    "    file=open(\"samsum_train.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Training file ID: {train_file.id}\")\n",
    "\n",
    "# Upload validation file\n",
    "print(\"Uploading validation file...\")\n",
    "val_file = client.files.create(\n",
    "    file=open(\"samsum_val.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Validation file ID: {val_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be00d8",
   "metadata": {},
   "source": [
    "## Create Fine-tuning Job\n",
    "\n",
    "Start the fine-tuning job with 1 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e02500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating fine-tuning job...\n",
      "Fine-tuning job created: ftjob-U26WNWTAuJGgNvTtKutPrw4B\n",
      "Status: validating_files\n",
      "Fine-tuning job created: ftjob-hAuLEXy1WdHqexGeY2u81z6F\n",
      "Status: validating_files\n",
      "Fine-tuning job created: ftjob-FV8juQuguXnTPECNPza85e11\n",
      "Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "# Create fine-tuning job\n",
    "print(\"\\nCreating fine-tuning job...\")\n",
    "job1 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 1,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "print(f\"Fine-tuning job created: {job1.id}\")\n",
    "print(f\"Status: {job1.status}\")\n",
    "\n",
    "\n",
    "job2 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 0.5,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "print(f\"Fine-tuning job created: {job2.id}\")\n",
    "print(f\"Status: {job2.status}\")\n",
    "\n",
    "\n",
    "job3 = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1,\n",
    "        \"learning_rate_multiplier\": 0.1,\n",
    "        \"batch_size\": 8,\n",
    "\n",
    "    },\n",
    "    suffix=\"samsum\"\n",
    ")\n",
    "print(f\"Fine-tuning job created: {job3.id}\")\n",
    "print(f\"Status: {job3.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e4b4e",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "Test the fine-tuned model on the validation set and compare to baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "786d7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_id):\n",
    "    \"\"\"Evaluate a model on the validation set and compute ROUGE.\"\"\"\n",
    "\n",
    "    def evaluate_sample(sample):\n",
    "        q = sample[\"dialogue\"]\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": build_user_prompt(q)}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    # Run samples in parallel but preserve order\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(executor.map(evaluate_sample, val_data), total=len(val_data)))\n",
    "\n",
    "    rouge = calculate_rouge(results, list(val_data[\"summary\"]))\n",
    "    return rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845c388",
   "metadata": {},
   "source": [
    "## lr_multiplier = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68eafed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:33<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.11296262397526508\n",
      "Rouge-2:  0.01201384855827984\n",
      "Rouge-L:  0.0882997597030008\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rgruH')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d38f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Victoria and Magda are both feeling financially strained this month. Magda just paid her car insurance, while Victoria's is paid for the rest of the year.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rgruH',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c910371b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victoria expresses her frustration about being broke after overspending, but looks forward to getting paid soon. Magda relates to her feelings and mentions just paying her car insurance, feeling like she was robbed. Victoria is relieved that her car insurance is already paid for the year.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d38353",
   "metadata": {},
   "source": [
    "## lr_multiplier = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c45ee459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:26<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.12924997802793536\n",
      "Rouge-2:  0.02352655682291644\n",
      "Rouge-L:  0.10569437415041297\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4qjCPA')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3093be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victoria and Magda are broke. Magda just paid her car insurance.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4qjCPA',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ace933",
   "metadata": {},
   "source": [
    "## lr_multiplier = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "190968ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:37<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1:  0.1149850396819559\n",
      "Rouge-2:  0.0124089839747357\n",
      "Rouge-L:  0.09043878235459887\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_model('ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rbTPa')\n",
    "\n",
    "print('Rouge-1: ', rouge_score['rouge1'])\n",
    "print('Rouge-2: ', rouge_score['rouge2'])\n",
    "print('Rouge-L: ', rouge_score['rougeL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03f977a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Victoria and Magda are broke. Victoria's car insurance is paid for the rest of the year.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='ft:gpt-4o-mini-2024-07-18:ready-tensor-inc:samsum:CZ4rbTPa',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that summarize conversations. Summarize the conversation in a few sentences.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Dialogue: {dialogue}\\nSummary:\"}\n",
    "        ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pred = completion.choices[0].message.content\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff980c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
