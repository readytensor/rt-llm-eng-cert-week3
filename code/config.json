{
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "assistant_only_masking": true,
    "use_qlora": true,
    "use_sagemaker": true,
    "quantization_config": {
        "load_in_4bit": true
    },
    "lora_config": {
        "r": 8,
        "lora_alpha": 32,
        "target_modules": [
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj"
        ],
        "lora_dropout": 0.05,
        "bias": "none"
    },
    "training_args": {
        "output_dir": "./checkpoints",
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 4,
        "num_train_epochs": 30,
        "learning_rate": 3e-3,
        "logging_steps": 1,
        "save_strategy": "epoch",
        "eval_strategy": "no",
        "warmup_steps": 2,
        "lr_scheduler_type": "cosine",
        "optim": "adamw_torch",
        "report_to": null,
        "remove_unused_columns": false,
        "dataloader_drop_last": true,
        "gradient_checkpointing": true
    }
}