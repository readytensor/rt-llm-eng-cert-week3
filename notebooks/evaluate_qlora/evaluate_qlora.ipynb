{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating QLoRA Fine-Tuned Models\n",
        "\n",
        "This notebook demonstrates how to evaluate a QLoRA fine-tuned model on dialogue summarization using the SAMSum dataset.\n",
        "\n",
        "**What we'll do:**\n",
        "- Load a fine-tuned model with LoRA adapters from Hugging Face Hub\n",
        "- Generate predictions on the validation set\n",
        "- Compute ROUGE metrics to measure performance\n",
        "- Save results and predictions for analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Install Dependencies\n",
        "\n",
        "Install the required packages for loading and evaluating QLoRA models, including PEFT for LoRA adapters and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh81P9ygpflZ",
        "outputId": "08f3f924-7281-4d7f-8c3f-192bade9346f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install -q torch transformers peft evaluate rouge_score\n",
        "! pip install bitsandbytes -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries\n",
        "\n",
        "Import necessary libraries for:\n",
        "- Model loading (transformers, PEFT)\n",
        "- Evaluation metrics (evaluate, ROUGE)\n",
        "- Data handling and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7x-f4rwoL4aE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from tqdm import tqdm\n",
        "from peft import PeftModel, get_peft_model, LoraConfig\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorWithPadding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Paths\n",
        "\n",
        "Set up directories for:\n",
        "- Configuration files\n",
        "- Output storage (predictions and metrics)\n",
        "- Dataset caching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ElrN1eiJp_jM"
      },
      "outputs": [],
      "source": [
        "CONFIG_FILE_PATH = './config.yaml'\n",
        "OUTPUTS_DIR = './outputs'\n",
        "DATASETS_DIR = './datasets'\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Utility Functions\n",
        "\n",
        "Core functions for:\n",
        "- **Loading configuration** from YAML\n",
        "- **Model setup** with 4-bit quantization (without training LoRA from scratch)\n",
        "- **Dataset loading** with subset selection\n",
        "- **Data preparation** for evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Vb9001tL0_i"
      },
      "outputs": [],
      "source": [
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"\n",
        "    Load and parse a YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the config file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
        "    \"\"\"\n",
        "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
        "\n",
        "    Args:\n",
        "        cfg (dict): Configuration dictionary containing:\n",
        "            - base_model\n",
        "            - quantization parameters\n",
        "            - lora parameters (optional)\n",
        "            - bf16 or fp16 precision\n",
        "        use_4bit (bool, optional): Override whether to load in 4-bit mode.\n",
        "        use_lora (bool, optional): Override whether to apply LoRA adapters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    model_name = cfg[\"base_model\"]\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Tokenizer setup\n",
        "    # ------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Determine quantization + LoRA usage\n",
        "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
        "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Quantization setup (optional)\n",
        "    # ------------------------------\n",
        "    quant_cfg = None\n",
        "    if load_in_4bit:\n",
        "        print(\"‚öôÔ∏è  Enabling 4-bit quantization...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
        "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
        "            bnb_4bit_compute_dtype=getattr(\n",
        "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚öôÔ∏è  Loading model in full precision (no quantization).\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Model loading\n",
        "    # ------------------------------\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_cfg,\n",
        "        device_map=\"auto\",\n",
        "        dtype=(\n",
        "            torch.bfloat16\n",
        "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
        "            else torch.float32\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ------------------------------\n",
        "    # LoRA setup (optional)\n",
        "    # ------------------------------\n",
        "    if apply_lora:\n",
        "        print(\"üîß Applying LoRA configuration...\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=cfg.get(\"lora_r\", 8),\n",
        "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
        "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
        "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        model.print_trainable_parameters()\n",
        "    else:\n",
        "        print(\"üîπ Skipping LoRA setup ‚Äî using base model only.\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"\n",
        "    Select a subset of the dataset.\n",
        "    If n_samples is \"all\" or None, return the entire dataset.\n",
        "    Otherwise, sample n_samples examples.\n",
        "    \"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"‚ö†Ô∏è  Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"\n",
        "    Load dataset splits according to configuration.\n",
        "    Ensures the FULL dataset is cached, and subsets are selected per run.\n",
        "    Supports both new-style (\"dataset\": {\"splits\": {...}}) and old-style (top-level keys) configs.\n",
        "    \"\"\"\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Extract dataset configuration\n",
        "    # -----------------------------------------------------------------------\n",
        "    if \"dataset\" in cfg:\n",
        "        cfg_dataset = cfg[\"dataset\"]\n",
        "        dataset_name = cfg_dataset[\"name\"]\n",
        "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
        "        n_train = splits_cfg.get(\"train\", \"all\")\n",
        "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
        "        n_test = splits_cfg.get(\"test\", \"all\")\n",
        "        seed = cfg_dataset.get(\"seed\", 42)\n",
        "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
        "        cfg_dataset = cfg[\"datasets\"][0]\n",
        "        dataset_name = cfg_dataset[\"path\"]\n",
        "        n_train = cfg.get(\"train_samples\", \"all\")\n",
        "        n_val = cfg.get(\"val_samples\", \"all\")\n",
        "        n_test = cfg.get(\"test_samples\", \"all\")\n",
        "        seed = cfg.get(\"seed\", 42)\n",
        "    else:\n",
        "        raise KeyError(\"Dataset configuration not found. Expected 'dataset' or 'datasets' key.\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Load or download full dataset\n",
        "    # -----------------------------------------------------------------------\n",
        "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"üìÇ Loading dataset from local cache: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "    else:\n",
        "        print(f\"‚¨áÔ∏è  Downloading dataset from Hugging Face: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        dataset.save_to_disk(local_path)\n",
        "        print(f\"‚úÖ Full dataset saved locally to: {local_path}\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Handle variations in split keys and select subsets dynamically\n",
        "    # -----------------------------------------------------------------------\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "\n",
        "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
        "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
        "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
        "\n",
        "    print(f\"üìä Loaded {len(train)} train / {len(val)} val / {len(test)} test samples (from full cache).\")\n",
        "    return train, val, test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inference & Evaluation Functions\n",
        "\n",
        "### Generation Function\n",
        "Generate summaries using the model in batches with the same prompt template used during training.\n",
        "\n",
        "### ROUGE Computation\n",
        "Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) to measure:\n",
        "- **ROUGE-1**: Unigram overlap (word-level similarity)\n",
        "- **ROUGE-2**: Bigram overlap (phrase-level similarity)  \n",
        "- **ROUGE-L**: Longest common subsequence (sentence structure similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GCrK32ZnL_X9"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate model predictions for a dataset (e.g., summaries).\n",
        "\n",
        "    Args:\n",
        "        model: The loaded model (base or fine-tuned).\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        dataset: Hugging Face dataset split containing 'dialogue' and 'summary'.\n",
        "        task_instruction (str): Instruction prefix for generation.\n",
        "        num_samples (int, optional): Number of samples to evaluate.\n",
        "        batch_size (int): Number of examples per inference batch.\n",
        "        max_new_tokens (int): Max tokens to generate per sample.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Generated summaries.\n",
        "    \"\"\"\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Prepare prompts\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        user_prompt = (\n",
        "            f\"{task_instruction}\\n\\n\"\n",
        "            f\"## Dialogue:\\n{sample['dialogue']}\\n\"\n",
        "            \"## Summary:\"\n",
        "        )\n",
        "        messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dtype=\"auto\",\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating summaries\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_rouge(predictions, samples):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores between predictions and reference summaries.\n",
        "\n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs.\n",
        "        samples (datasets.Dataset): Dataset containing reference 'summary' field.\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    references = [s[\"summary\"] for s in samples]\n",
        "    return rouge.compute(predictions=predictions, references=references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Main Evaluation Pipeline\n",
        "\n",
        "This function orchestrates the entire evaluation process:\n",
        "\n",
        "1. **Load base model** in 4-bit quantization\n",
        "2. **Attach LoRA adapters** from Hugging Face Hub\n",
        "3. **Load validation dataset** (same split used during training)\n",
        "4. **Generate predictions** for all validation samples\n",
        "5. **Compute ROUGE metrics** comparing predictions to references\n",
        "6. **Save results** (metrics + predictions) for analysis\n",
        "\n",
        "**Note:** We load the base model and attach pre-trained adapters rather than loading a fully merged model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FjT_HtAPqI1X"
      },
      "outputs": [],
      "source": [
        "def evaluate_peft_model(cfg, adapters_name):\n",
        "    \"\"\"Load base model, attach LoRA adapters, and evaluate.\"\"\"\n",
        "\n",
        "    # ----------------------------\n",
        "    # Model & Tokenizer\n",
        "    # ----------------------------\n",
        "    print(\"\\nLoading base model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(cfg, use_4bit=True, use_lora=False)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = PeftModel.from_pretrained(model, adapters_name).to(device)\n",
        "    model.eval()\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Dataset\n",
        "    # ----------------------------\n",
        "    print(\"\\nLoading dataset...\")\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "    print(f\"Validation set size: {len(val_data)} samples\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Inference\n",
        "    # ----------------------------\n",
        "    print(\"\\nGenerating summaries...\")\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        batch_size=cfg.get(\"eval_batch_size\", 4),\n",
        "    )\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluation\n",
        "    # ----------------------------\n",
        "    print(\"\\nComputing ROUGE metrics...\")\n",
        "    scores = compute_rouge(preds, val_data)\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  ROUGE-1: {scores['rouge1']:.2%}\")\n",
        "    print(f\"  ROUGE-2: {scores['rouge2']:.2%}\")\n",
        "    print(f\"  ROUGE-L: {scores['rougeL']:.2%}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Save Outputs\n",
        "    # ----------------------------\n",
        "    output_dir = os.path.join(OUTPUTS_DIR, \"lora_samsum\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    results_path = os.path.join(output_dir, \"eval_results.json\")\n",
        "    preds_path = os.path.join(output_dir, \"predictions.jsonl\")\n",
        "\n",
        "    results = {\n",
        "        \"rouge1\": scores[\"rouge1\"],\n",
        "        \"rouge2\": scores[\"rouge2\"],\n",
        "        \"rougeL\": scores[\"rougeL\"],\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"base_model\": cfg[\"base_model\"],\n",
        "    }\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, pred in enumerate(preds):\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"dialogue\": val_data[i][\"dialogue\"],\n",
        "                    \"reference\": val_data[i][\"summary\"],\n",
        "                    \"prediction\": pred,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"\\nSaved results to {results_path}\")\n",
        "    print(f\"Saved predictions to {preds_path}\")\n",
        "\n",
        "    return scores, preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Evaluation üöÄ\n",
        "\n",
        "Load your fine-tuned model adapters from Hugging Face Hub and evaluate on the validation set.\n",
        "\n",
        "**Expected Outputs:**\n",
        "- ROUGE scores printed to console\n",
        "- `eval_results.json` - Metrics summary\n",
        "- `predictions.jsonl` - All predictions with dialogues and references\n",
        "\n",
        "**Tip:** Compare these scores against the baseline model to measure improvement from fine-tuning!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPD7IbJBroHW",
        "outputId": "d716231f-2520-449e-b914-e1877cc364c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading base model...\n",
            "\n",
            "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
            "‚öôÔ∏è  Enabling 4-bit quantization...\n",
            "üîπ Skipping LoRA setup ‚Äî using base model only.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading dataset...\n",
            "üìÇ Loading dataset from local cache: ./datasets/knkarthick_samsum\n",
            "üìä Loaded 14731 train / 200 val / 200 test samples (from full cache).\n",
            "Validation set size: 200 samples\n",
            "\n",
            "Generating summaries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:03<00:00,  3.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Computing ROUGE metrics...\n",
            "\n",
            "Evaluation Results:\n",
            "  ROUGE-1: 47.33%\n",
            "  ROUGE-2: 22.77%\n",
            "  ROUGE-L: 39.13%\n",
            "\n",
            "Saved results to ./outputs/lora_samsum/eval_results.json\n",
            "Saved predictions to ./outputs/lora_samsum/predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "HF_USERNAME = userdata.get('HF_USERNAME')\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "scores, preds = evaluate_peft_model(cfg, f'{HF_USERNAME}/Llama-3.2-1B-QLoRA-Summarizer-adapters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Evaluation Complete!\n",
        "\n",
        "**Next Steps:**\n",
        "1. **Analyze predictions** - Review `predictions.jsonl` to see where the model succeeds/fails\n",
        "2. **Compare with baseline** - Look at ROUGE scores from the baseline model evaluation\n",
        "3. **Error analysis** - Identify patterns in low-quality summaries\n",
        "4. **Iterate** - Adjust hyperparameters (LoRA rank, learning rate) and retrain if needed\n",
        "\n",
        "**Key Metrics Interpretation:**\n",
        "- **ROUGE-1 > 40%** ‚Üí Good word-level coverage\n",
        "- **ROUGE-2 > 20%** ‚Üí Captures key phrases well\n",
        "- **ROUGE-L > 35%** ‚Üí Maintains sentence structure\n",
        "\n",
        "Typical improvements from QLoRA fine-tuning: **+5-15%** on ROUGE scores compared to zero-shot baseline!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
