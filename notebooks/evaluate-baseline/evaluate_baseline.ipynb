{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73828e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1480d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81981770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(dataset, n_samples, seed=42):\n",
    "    \"\"\"\n",
    "    Select a subset of the dataset.\n",
    "    If n_samples is \"all\" or None, return the entire dataset.\n",
    "    Otherwise, sample n_samples examples.\n",
    "    \"\"\"\n",
    "    if n_samples == \"all\" or n_samples is None:\n",
    "        return dataset\n",
    "    \n",
    "    if n_samples > len(dataset):\n",
    "        print(f\"‚ö†Ô∏è  Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
    "        return dataset\n",
    "    \n",
    "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
    "\n",
    "\n",
    "def load_and_prepare_dataset(cfg):\n",
    "    \"\"\"\n",
    "    Load dataset splits according to configuration.\n",
    "    Ensures the FULL dataset is cached, and subsets are selected per run.\n",
    "    Supports both new-style (\"dataset\": {\"splits\": {...}}) and old-style (top-level keys) configs.\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Extract dataset configuration\n",
    "    # -----------------------------------------------------------------------\n",
    "    if \"dataset\" in cfg:\n",
    "        cfg_dataset = cfg[\"dataset\"]\n",
    "        dataset_name = cfg_dataset[\"name\"]\n",
    "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
    "        n_train = splits_cfg.get(\"train\", \"all\")\n",
    "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
    "        n_test = splits_cfg.get(\"test\", \"all\")\n",
    "        seed = cfg_dataset.get(\"seed\", 42)\n",
    "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
    "        cfg_dataset = cfg[\"datasets\"][0]\n",
    "        dataset_name = cfg_dataset[\"path\"]\n",
    "        n_train = cfg.get(\"train_samples\", \"all\")\n",
    "        n_val = cfg.get(\"val_samples\", \"all\")\n",
    "        n_test = cfg.get(\"test_samples\", \"all\")\n",
    "        seed = cfg.get(\"seed\", 42)\n",
    "    else:\n",
    "        raise KeyError(\"Dataset configuration not found. Expected 'dataset' or 'datasets' key.\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Load or download full dataset\n",
    "    # -----------------------------------------------------------------------\n",
    "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
    "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
    "\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"üìÇ Loading dataset from local cache: {local_path}\")\n",
    "        dataset = load_from_disk(local_path)\n",
    "    else:\n",
    "        print(f\"‚¨áÔ∏è  Downloading dataset from Hugging Face: {dataset_name}\")\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        dataset.save_to_disk(local_path)\n",
    "        print(f\"‚úÖ Full dataset saved locally to: {local_path}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Handle variations in split keys and select subsets dynamically\n",
    "    # -----------------------------------------------------------------------\n",
    "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
    "\n",
    "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
    "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
    "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
    "\n",
    "    print(f\"üìä Loaded {len(train)} train / {len(val)} val / {len(test)} test samples (from full cache).\")\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655fed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
    "    \"\"\"\n",
    "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): Configuration dictionary containing:\n",
    "            - base_model\n",
    "            - quantization parameters\n",
    "            - lora parameters (optional)\n",
    "            - bf16 or fp16 precision\n",
    "        use_4bit (bool, optional): Override whether to load in 4-bit mode.\n",
    "        use_lora (bool, optional): Override whether to apply LoRA adapters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    model_name = cfg[\"base_model\"]\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Tokenizer setup\n",
    "    # ------------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Determine quantization + LoRA usage\n",
    "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
    "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Quantization setup (optional)\n",
    "    # ------------------------------\n",
    "    quant_cfg = None\n",
    "    if load_in_4bit:\n",
    "        print(\"‚öôÔ∏è  Enabling 4-bit quantization...\")\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
    "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
    "            bnb_4bit_compute_dtype=getattr(\n",
    "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚öôÔ∏è  Loading model in full precision (no quantization).\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Model loading\n",
    "    # ------------------------------\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=\"auto\",\n",
    "        dtype=(\n",
    "            torch.bfloat16\n",
    "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
    "            else torch.float32\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # LoRA setup (optional)\n",
    "    # ------------------------------\n",
    "    if apply_lora:\n",
    "        print(\"üîß Applying LoRA configuration...\")\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=cfg.get(\"lora_r\", 8),\n",
    "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
    "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
    "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        print(\"üîπ Skipping LoRA setup ‚Äî using base model only.\")\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "660ef9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"../code/config.yaml\"\n",
    "\n",
    "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
    "    \"\"\"\n",
    "    Load and parse a YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the config file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration dictionary.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b9df4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    task_instruction,\n",
    "    num_samples=None,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate model predictions for a dataset (e.g., summaries).\n",
    "\n",
    "    Args:\n",
    "        model: The loaded model (base or fine-tuned).\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        dataset: Hugging Face dataset split containing 'dialogue' and 'summary'.\n",
    "        task_instruction (str): Instruction prefix for generation.\n",
    "        num_samples (int, optional): Number of samples to evaluate.\n",
    "        batch_size (int): Number of examples per inference batch.\n",
    "        max_new_tokens (int): Max tokens to generate per sample.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Generated summaries.\n",
    "    \"\"\"\n",
    "    if num_samples is not None and num_samples < len(dataset):\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "\n",
    "    # Prepare prompts\n",
    "    prompts = []\n",
    "    for sample in dataset:\n",
    "        user_prompt = (\n",
    "            f\"{task_instruction}\\n\\n\"\n",
    "            f\"## Dialogue:\\n{sample['dialogue']}\\n\"\n",
    "            \"## Summary:\"\n",
    "        )\n",
    "        messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Initialize pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dtype=\"auto\",\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating summaries\"):\n",
    "        batch = prompts[i : i + batch_size]\n",
    "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
    "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def compute_rouge(predictions, samples):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores between predictions and reference summaries.\n",
    "\n",
    "    Args:\n",
    "        predictions (list[str]): Model-generated outputs.\n",
    "        samples (datasets.Dataset): Dataset containing reference 'summary' field.\n",
    "\n",
    "    Returns:\n",
    "        dict: ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    references = [s[\"summary\"] for s in samples]\n",
    "    return rouge.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24340d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting baseline evaluation...\n",
      "‚¨áÔ∏è  Downloading dataset from Hugging Face: knkarthick/samsum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d661423a34432380a01f5ae57fa0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc690e62f8c43f2a92342aae4ae7a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9bdd1474234245b760941338219fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full dataset saved locally to: ../datasets/knkarthick_samsum\n",
      "üìä Loaded 14731 train / 200 val / 200 test samples (from full cache).\n",
      "üìä Loaded 200 validation samples.\n",
      "\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "‚öôÔ∏è  Loading model in full precision (no quantization).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Skipping LoRA setup ‚Äî using base model only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:53<00:00,  4.66s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Starting baseline evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     rouge_scores, predictions = \u001b[43mevaluate_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Evaluation complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìà Baseline ROUGE Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mevaluate_baseline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m preds_path = \u001b[33m\"\u001b[39m\u001b[33mpredictions.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mjson\u001b[49m.dump(results, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(preds_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds):\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluate_baseline.py\n",
    "Evaluate the base (unfine-tuned) model on the SAMSum dataset to establish baseline ROUGE scores.\n",
    "\"\"\"\n",
    "\n",
    "cfg = load_config()\n",
    "\n",
    "def evaluate_baseline():\n",
    "    \"\"\"Run baseline evaluation on the SAMSum dataset using the base model.\"\"\"\n",
    "\n",
    "    # Load validation data\n",
    "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
    "    print(f\"üìä Loaded {len(val_data)} validation samples.\")\n",
    "\n",
    "    # Load model + tokenizer (no quantization or LoRA)\n",
    "    model, tokenizer = setup_model_and_tokenizer(\n",
    "        cfg=cfg,\n",
    "        use_4bit=False,\n",
    "        use_lora=False,\n",
    "    )\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = generate_predictions(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=val_data,\n",
    "        task_instruction=cfg[\"task_instruction\"],\n",
    "        batch_size=4,\n",
    "    )\n",
    "\n",
    "    # Compute ROUGE metrics\n",
    "    scores = compute_rouge(preds, val_data)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Save outputs\n",
    "    # -----------------------------------------------------------------------\n",
    "    results = {\n",
    "        \"model_name\": cfg[\"base_model\"],\n",
    "        \"num_samples\": len(val_data),\n",
    "        \"rouge1\": scores[\"rouge1\"],\n",
    "        \"rouge2\": scores[\"rouge2\"],\n",
    "        \"rougeL\": scores[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "    results_path = \"eval_results.json\"\n",
    "    preds_path = \"predictions.jsonl\"\n",
    "\n",
    "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, pred in enumerate(preds):\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"dialogue\": val_data[i][\"dialogue\"],\n",
    "                    \"reference\": val_data[i][\"summary\"],\n",
    "                    \"prediction\": pred,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"\\nüíæ Saved results to: {results_path}\")\n",
    "    print(f\"üíæ Saved predictions to: {preds_path}\")\n",
    "\n",
    "    return scores, preds\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main\n",
    "# ---------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting baseline evaluation...\")\n",
    "    rouge_scores, predictions = evaluate_baseline()\n",
    "    print(\"\\n‚úÖ Evaluation complete.\")\n",
    "\n",
    "    \n",
    "    print(\"\\nüìà Baseline ROUGE Results:\")\n",
    "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.2%}\")\n",
    "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.2%}\")\n",
    "    print(f\"  ROUGE-L: {rouge_scores['rougeL']:.2%}\")\n",
    "\n",
    "    print(\"\\nExample prediction:\\n\")\n",
    "    print(predictions[0])\n",
    "    print(\"\\nRouge scores:\\n\")\n",
    "    print(rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfede576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
