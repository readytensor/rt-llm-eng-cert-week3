{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q evaluate torch tqdm datasets peft transformers rouge_score\n",
        "! pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-HKFuB4zKpk",
        "outputId": "a68dd92f-704b-4fb4-dbb8-5383d7f95f3b"
      },
      "id": "c-HKFuB4zKpk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73828e7d",
      "metadata": {
        "id": "73828e7d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1480d0",
      "metadata": {
        "id": "1e1480d0"
      },
      "outputs": [],
      "source": [
        "DATASETS_DIR = \"./datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81981770",
      "metadata": {
        "id": "81981770"
      },
      "outputs": [],
      "source": [
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"\n",
        "    Select a subset of the dataset.\n",
        "    If n_samples is \"all\" or None, return the entire dataset.\n",
        "    Otherwise, sample n_samples examples.\n",
        "    \"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"‚ö†Ô∏è  Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"\n",
        "    Load dataset splits according to configuration.\n",
        "    Ensures the FULL dataset is cached, and subsets are selected per run.\n",
        "    Supports both new-style (\"dataset\": {\"splits\": {...}}) and old-style (top-level keys) configs.\n",
        "    \"\"\"\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Extract dataset configuration\n",
        "    # -----------------------------------------------------------------------\n",
        "    if \"dataset\" in cfg:\n",
        "        cfg_dataset = cfg[\"dataset\"]\n",
        "        dataset_name = cfg_dataset[\"name\"]\n",
        "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
        "        n_train = splits_cfg.get(\"train\", \"all\")\n",
        "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
        "        n_test = splits_cfg.get(\"test\", \"all\")\n",
        "        seed = cfg_dataset.get(\"seed\", 42)\n",
        "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
        "        cfg_dataset = cfg[\"datasets\"][0]\n",
        "        dataset_name = cfg_dataset[\"path\"]\n",
        "        n_train = cfg.get(\"train_samples\", \"all\")\n",
        "        n_val = cfg.get(\"val_samples\", \"all\")\n",
        "        n_test = cfg.get(\"test_samples\", \"all\")\n",
        "        seed = cfg.get(\"seed\", 42)\n",
        "    else:\n",
        "        raise KeyError(\"Dataset configuration not found. Expected 'dataset' or 'datasets' key.\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Load or download full dataset\n",
        "    # -----------------------------------------------------------------------\n",
        "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"üìÇ Loading dataset from local cache: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "    else:\n",
        "        print(f\"‚¨áÔ∏è  Downloading dataset from Hugging Face: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        dataset.save_to_disk(local_path)\n",
        "        print(f\"‚úÖ Full dataset saved locally to: {local_path}\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Handle variations in split keys and select subsets dynamically\n",
        "    # -----------------------------------------------------------------------\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "\n",
        "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
        "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
        "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
        "\n",
        "    print(f\"üìä Loaded {len(train)} train / {len(val)} val / {len(test)} test samples (from full cache).\")\n",
        "    return train, val, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655fed70",
      "metadata": {
        "id": "655fed70"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
        "    \"\"\"\n",
        "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
        "\n",
        "    Args:\n",
        "        cfg (dict): Configuration dictionary containing:\n",
        "            - base_model\n",
        "            - quantization parameters\n",
        "            - lora parameters (optional)\n",
        "            - bf16 or fp16 precision\n",
        "        use_4bit (bool, optional): Override whether to load in 4-bit mode.\n",
        "        use_lora (bool, optional): Override whether to apply LoRA adapters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    model_name = cfg[\"base_model\"]\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Tokenizer setup\n",
        "    # ------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Determine quantization + LoRA usage\n",
        "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
        "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Quantization setup (optional)\n",
        "    # ------------------------------\n",
        "    quant_cfg = None\n",
        "    if load_in_4bit:\n",
        "        print(\"‚öôÔ∏è  Enabling 4-bit quantization...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
        "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
        "            bnb_4bit_compute_dtype=getattr(\n",
        "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚öôÔ∏è  Loading model in full precision (no quantization).\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Model loading\n",
        "    # ------------------------------\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_cfg,\n",
        "        device_map=\"auto\",\n",
        "        dtype=(\n",
        "            torch.bfloat16\n",
        "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
        "            else torch.float32\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ------------------------------\n",
        "    # LoRA setup (optional)\n",
        "    # ------------------------------\n",
        "    if apply_lora:\n",
        "        print(\"üîß Applying LoRA configuration...\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=cfg.get(\"lora_r\", 8),\n",
        "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
        "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
        "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        model.print_trainable_parameters()\n",
        "    else:\n",
        "        print(\"üîπ Skipping LoRA setup ‚Äî using base model only.\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660ef9fe",
      "metadata": {
        "id": "660ef9fe"
      },
      "outputs": [],
      "source": [
        "CONFIG_FILE_PATH = \"./config.yaml\"\n",
        "\n",
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"\n",
        "    Load and parse a YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the config file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9df4e2",
      "metadata": {
        "id": "7b9df4e2"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate model predictions for a dataset (e.g., summaries).\n",
        "\n",
        "    Args:\n",
        "        model: The loaded model (base or fine-tuned).\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        dataset: Hugging Face dataset split containing 'dialogue' and 'summary'.\n",
        "        task_instruction (str): Instruction prefix for generation.\n",
        "        num_samples (int, optional): Number of samples to evaluate.\n",
        "        batch_size (int): Number of examples per inference batch.\n",
        "        max_new_tokens (int): Max tokens to generate per sample.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Generated summaries.\n",
        "    \"\"\"\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Prepare prompts\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        user_prompt = (\n",
        "            f\"{task_instruction}\\n\\n\"\n",
        "            f\"## Dialogue:\\n{sample['dialogue']}\\n\"\n",
        "            \"## Summary:\"\n",
        "        )\n",
        "        messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dtype=\"auto\",\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating summaries\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_rouge(predictions, samples):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores between predictions and reference summaries.\n",
        "\n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs.\n",
        "        samples (datasets.Dataset): Dataset containing reference 'summary' field.\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    references = [s[\"summary\"] for s in samples]\n",
        "    return rouge.compute(predictions=predictions, references=references)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24340d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24340d4c",
        "outputId": "01705fc7-8b8b-4031-8ccd-600e2b914547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting baseline evaluation...\n",
            "üìÇ Loading dataset from local cache: ./datasets/knkarthick_samsum\n",
            "üìä Loaded 14731 train / 200 val / 200 test samples (from full cache).\n",
            "üìä Loaded 200 validation samples.\n",
            "\n",
            "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
            "‚öôÔ∏è  Loading model in full precision (no quantization).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Skipping LoRA setup ‚Äî using base model only.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:38<00:00,  3.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saved results to: eval_results.json\n",
            "üíæ Saved predictions to: predictions.jsonl\n",
            "\n",
            "‚úÖ Evaluation complete.\n",
            "\n",
            "üìà Baseline ROUGE Results:\n",
            "  ROUGE-1: 35.46%\n",
            "  ROUGE-2: 11.18%\n",
            "  ROUGE-L: 26.56%\n",
            "\n",
            "Example prediction:\n",
            "\n",
            "Victoria and Magda commiserated about their financial struggles, with Victoria expressing frustration about overspending, and Magda jokingly commiserating about her car insurance being paid for the rest of the year.\n",
            "\n",
            "Rouge scores:\n",
            "\n",
            "{'rouge1': np.float64(0.3545633958887797), 'rouge2': np.float64(0.1117964129289076), 'rougeL': np.float64(0.26562850525807696), 'rougeLsum': np.float64(0.2660814302845086)}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "evaluate_baseline.py\n",
        "Evaluate the base (unfine-tuned) model on the SAMSum dataset to establish baseline ROUGE scores.\n",
        "\"\"\"\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "def evaluate_baseline():\n",
        "    \"\"\"Run baseline evaluation on the SAMSum dataset using the base model.\"\"\"\n",
        "\n",
        "    # Load validation data\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "    print(f\"üìä Loaded {len(val_data)} validation samples.\")\n",
        "\n",
        "    # Load model + tokenizer (no quantization or LoRA)\n",
        "    model, tokenizer = setup_model_and_tokenizer(\n",
        "        cfg=cfg,\n",
        "        use_4bit=False,\n",
        "        use_lora=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        batch_size=4,\n",
        "    )\n",
        "\n",
        "    # Compute ROUGE metrics\n",
        "    scores = compute_rouge(preds, val_data)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Save outputs\n",
        "    # -----------------------------------------------------------------------\n",
        "    results = {\n",
        "        \"model_name\": cfg[\"base_model\"],\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"rouge1\": scores[\"rouge1\"],\n",
        "        \"rouge2\": scores[\"rouge2\"],\n",
        "        \"rougeL\": scores[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "    results_path = \"eval_results.json\"\n",
        "    preds_path = \"predictions.jsonl\"\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, pred in enumerate(preds):\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"dialogue\": val_data[i][\"dialogue\"],\n",
        "                    \"reference\": val_data[i][\"summary\"],\n",
        "                    \"prediction\": pred,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"\\nüíæ Saved results to: {results_path}\")\n",
        "    print(f\"üíæ Saved predictions to: {preds_path}\")\n",
        "\n",
        "    return scores, preds\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main\n",
        "# ---------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting baseline evaluation...\")\n",
        "    rouge_scores, predictions = evaluate_baseline()\n",
        "    print(\"\\n‚úÖ Evaluation complete.\")\n",
        "\n",
        "\n",
        "    print(\"\\nüìà Baseline ROUGE Results:\")\n",
        "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.2%}\")\n",
        "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.2%}\")\n",
        "    print(f\"  ROUGE-L: {rouge_scores['rougeL']:.2%}\")\n",
        "\n",
        "    print(\"\\nExample prediction:\\n\")\n",
        "    print(predictions[0])\n",
        "    print(\"\\nRouge scores:\\n\")\n",
        "    print(rouge_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfede576",
      "metadata": {
        "id": "cfede576"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}