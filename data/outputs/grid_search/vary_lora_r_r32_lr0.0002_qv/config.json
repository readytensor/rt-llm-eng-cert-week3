{
  "base_model": "meta-llama/Llama-3.2-1B-Instruct",
  "tokenizer_type": "meta-llama/Llama-3.2-1B-Instruct",
  "dataset": {
    "name": "knkarthick/samsum",
    "cache_dir": "../data/datasets",
    "field_map": {
      "input": "dialogue",
      "output": "summary"
    },
    "type": "completion",
    "splits": {
      "train": "all",
      "validation": 200,
      "test": 200
    },
    "seed": 42
  },
  "datasets": [
    {
      "path": "knkarthick/samsum",
      "cache_dir": "../data/datasets",
      "field_map": {
        "input": "dialogue",
        "output": "summary"
      },
      "type": "completion"
    }
  ],
  "task_instruction": "You are a helpful assistant who writes concise, factual summaries of conversations. Summarize the following conversation into a single sentence.\n",
  "train_samples": "all",
  "val_samples": 200,
  "test_samples": 200,
  "seed": 42,
  "load_in_4bit": true,
  "bnb_4bit_quant_type": "nf4",
  "bnb_4bit_use_double_quant": true,
  "bnb_4bit_compute_dtype": "bfloat16",
  "lora_r": 32,
  "lora_alpha": 64,
  "lora_dropout": 0.1,
  "target_modules": [
    "q_proj",
    "v_proj"
  ],
  "num_epochs": 1,
  "max_steps": 300,
  "learning_rate": 0.0002,
  "batch_size": 4,
  "gradient_accumulation_steps": 4,
  "sequence_len": 512,
  "lr_scheduler": "cosine",
  "warmup_steps": 50,
  "bf16": true,
  "logging_steps": 25,
  "save_steps": 100,
  "save_total_limit": 2,
  "optim": "paged_adamw_8bit",
  "output_dir": "./outputs/lora_samsum",
  "wandb_project": "llama3_samsum",
  "wandb_run_name": "vary_lora_r_r32_lr0.0002_qv",
  "save_dir": "/workspace/rt-llm-eng-cert-week3/data/outputs/grid_search/vary_lora_r_r32_lr0.0002_qv"
}