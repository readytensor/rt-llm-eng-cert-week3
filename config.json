{
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "save_model_name": "llama-1b-legal-qlora",
    "assistant_only_masking": true,
    "use_qlora": true,
    "dataset_config": {
        "dataset_name": "NebulaSense/Legal_Clause_Instructions",
        "instruction_column": "Instruction",
        "input_column": "Input",
        "output_column": "Output",
        "max_length": 2048,
        "sample_size": 1000
    },
    "quantization_config": {
        "load_in_4bit": true
    },
    "lora_config": {
        "r": 8,
        "lora_alpha": 32,
        "target_modules": [
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj"
        ],
        "lora_dropout": 0.05,
        "bias": "none"
    },
    "training_args": {
        "output_dir": "./checkpoints",
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "num_train_epochs": 3,
        "learning_rate": 5e-4,
        "logging_steps": 4,
        "save_strategy": "epoch",
        "eval_strategy": "no",
        "warmup_steps": 0,
        "lr_scheduler_type": "cosine",
        "optim": "adamw_torch",
        "report_to": null,
        "remove_unused_columns": false,
        "dataloader_drop_last": true,
        "gradient_checkpointing": false
    }
}